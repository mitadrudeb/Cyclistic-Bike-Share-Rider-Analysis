{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96aa3599",
   "metadata": {},
   "source": [
    "# Cyclistic Bike-Share Analysis\n",
    "\n",
    "## Phase 3: PROCESS\n",
    "\n",
    "## Data Cleaning and Transformation\n",
    "\n",
    "In this notebook, I'm documenting my systematic approach to cleaning and preparing the combined 2024 dataset. This is a critical step to ensure my analysis is based on accurate, reliable data.\n",
    "\n",
    "**Input:** combined_2024_raw.csv (5,860,568 rows x 13 columns)  \n",
    "**Objective:** Create a clean, analysis-ready dataset\n",
    "\n",
    "## My Cleaning Strategy\n",
    "\n",
    "I will follow these steps:\n",
    "1. Load the combined raw data\n",
    "2. Handle missing values\n",
    "3. Remove duplicates\n",
    "4. Fix data types (convert datetime columns)\n",
    "5. Create calculated fields (ride_length, day_of_week)\n",
    "6. Remove invalid records\n",
    "7. Save cleaned dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d772ba9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 1: Load Combined Dataset\n",
    "\n",
    "I'll start by loading the raw combined dataset that I created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1bf7ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n",
      "____________________________________________________________\n",
      "Shape: (5860568,) rows x 13 columns\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Turn off warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the combined raw data from the previous step\n",
    "df = pd.read_csv('../data/raw/combined_2024_raw.csv')\n",
    "\n",
    "# Confirm the file loaded successfully\n",
    "print('Dataset loaded successfully')\n",
    "print('_' * 60)\n",
    "\n",
    "# Display the shape of the data\n",
    "total_rows = df.shape[0]\n",
    "total_columns = df.shape[1]\n",
    "print(f\"Shape: {total_rows,} rows x {total_columns} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82af23d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Names:\n",
      "------------------------------------------------------------\n",
      "1. ride_id\n",
      "2. rideable_type\n",
      "3. started_at\n",
      "4. ended_at\n",
      "5. start_station_name\n",
      "6. start_station_id\n",
      "7. end_station_name\n",
      "8. end_station_id\n",
      "9. start_lat\n",
      "10. start_lng\n",
      "11. end_lat\n",
      "12. end_lng\n",
      "13. member_casual\n",
      "\n",
      "Sample Data:\n",
      "------------------------------------------------------------\n",
      "            ride_id  rideable_type           started_at             ended_at  \\\n",
      "0  C1D650626C8C899A  electric_bike  2024-01-12 15:30:27  2024-01-12 15:37:59   \n",
      "1  EECD38BDB25BFCB0  electric_bike  2024-01-08 15:45:46  2024-01-08 15:52:59   \n",
      "2  F4A9CE78061F17F7  electric_bike  2024-01-27 12:27:19  2024-01-27 12:35:19   \n",
      "\n",
      "  start_station_name start_station_id          end_station_name  \\\n",
      "0  Wells St & Elm St     KA1504000135  Kingsbury St & Kinzie St   \n",
      "1  Wells St & Elm St     KA1504000135  Kingsbury St & Kinzie St   \n",
      "2  Wells St & Elm St     KA1504000135  Kingsbury St & Kinzie St   \n",
      "\n",
      "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \n",
      "0   KA1503000043  41.903267 -87.634737  41.889177 -87.638506        member  \n",
      "1   KA1503000043  41.902937 -87.634440  41.889177 -87.638506        member  \n",
      "2   KA1503000043  41.902951 -87.634470  41.889177 -87.638506        member  \n"
     ]
    }
   ],
   "source": [
    "# Display all column names\n",
    "print(\"\\nColumn Names:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Loop through each column and print its name with a number\n",
    "column_number = 1\n",
    "for column_name in df.columns:\n",
    "    print(f'{column_number}. {column_name}')\n",
    "    column_number = column_number + 1\n",
    "\n",
    "# Show a sample of the data\n",
    "print(\"\\nSample Data:\")\n",
    "print(\"-\" * 60)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2889e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Check and Remove Duplicates\n",
    "\n",
    "I need to check if there are any duplicate ride IDs in the dataset. Each ride should have a unique ID, so duplicates need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e944e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicates...\n",
      "------------------------------------------------------------\n",
      "Duplicate ride_ids found: 211\n",
      "Removing 211 duplicate records....\n",
      "New shape: 5,860,357 rows x 13 columns\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate ride_ids\n",
    "print(\"Checking for duplicates...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Count how many duplicate ride_ids exist\n",
    "duplicate_count = df['ride_id'].duplicated().sum()\n",
    "print(f\"Duplicate ride_ids found: {duplicate_count}\")\n",
    "\n",
    "# If duplicates exist, remove them\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Removing {duplicate_count} duplicate records....\")\n",
    "    # Keep the first occurrence and remove the rest\n",
    "    df = df.drop_duplicates(subset=['ride_id'], keep='first')\n",
    "    print(f\"New shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"No duplicates found. Data integrity confirmed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31abf75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3: Fix Data Types - Convert Datetime Columns\n",
    "\n",
    "The started_at and ended_at columns are currently stored as text strings. I need to convert them to datetime format so I can perform time-based calculations and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c48491b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datetime columns...\n",
      "------------------------------------------------------------\n",
      "Datetime conversion complete\n",
      "\n",
      "Data types after conversion:\n",
      "started_at    datetime64[ns]\n",
      "ended_at      datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert started_at and ended_at columns from text to datetime format\n",
    "print(\"Converting datetime columns...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Convert started_at column to datetime\n",
    "df['started_at'] = pd.to_datetime(df['started_at'], format='mixed')\n",
    "\n",
    "# Convert ended_at column to datetime\n",
    "df['ended_at'] = pd.to_datetime(df['ended_at'], format='mixed')\n",
    "\n",
    "# Confirm conversion was successful\n",
    "print(\"Datetime conversion complete\")\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(df[['started_at', 'ended_at']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b719d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Create Calculated Fields\n",
    "\n",
    "I'm creating new columns that will be useful for my analysis:\n",
    "- **ride_length:** Duration of each ride in minutes\n",
    "- **day_of_week:** Day when ride started (Monday=0, Sunday=6)\n",
    "- **month:** Month when ride started\n",
    "- **hour:** Hour when ride started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "226532aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating calculated fields...\n",
      "------------------------------------------------------------\n",
      "Calculated fields created successfully\n",
      "\n",
      "New columns:\n",
      "           started_at            ended_at  ride_length  day_of_week  month  \\\n",
      "0 2024-01-12 15:30:27 2024-01-12 15:37:59     7.533333            4      1   \n",
      "1 2024-01-08 15:45:46 2024-01-08 15:52:59     7.216667            0      1   \n",
      "2 2024-01-27 12:27:19 2024-01-27 12:35:19     8.000000            5      1   \n",
      "3 2024-01-29 16:26:17 2024-01-29 16:56:06    29.816667            0      1   \n",
      "4 2024-01-31 05:43:23 2024-01-31 06:09:35    26.200000            2      1   \n",
      "\n",
      "   hour  \n",
      "0    15  \n",
      "1    15  \n",
      "2    12  \n",
      "3    16  \n",
      "4     5  \n"
     ]
    }
   ],
   "source": [
    "# Create calculated fields\n",
    "print(\"Creating calculated fields...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calculate ride_length in minutes\n",
    "# First, calculate the time difference\n",
    "time_difference = df['ended_at'] - df['started_at']\n",
    "\n",
    "# Convert to total seconds\n",
    "time_in_seconds = time_difference.dt.total_seconds()\n",
    "\n",
    "# Convert seconds to minutes\n",
    "df['ride_length'] = time_in_seconds / 60\n",
    "\n",
    "# Extract day of week (0=Monday, 6=Sunday)\n",
    "df['day_of_week'] = df['started_at'].dt.dayofweek\n",
    "\n",
    "# Extract month\n",
    "df['month'] = df['started_at'].dt.month\n",
    "\n",
    "# Extract hour\n",
    "df['hour'] = df['started_at'].dt.hour\n",
    "\n",
    "# Confirm new columns were created\n",
    "print(\"Calculated fields created successfully\")\n",
    "print(\"\\nNew columns:\")\n",
    "print(df[['started_at', 'ended_at', 'ride_length', 'day_of_week', 'month', 'hour']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f57d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 5: Analyze Missing Values\n",
    "\n",
    "Now I need to understand which columns have missing values and decide how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e1ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Analysis:\n",
      "------------------------------------------------------------\n",
      "            Column  Missing_Count  Missing_Percentage\n",
      "    end_station_id        1104579               18.85\n",
      "  end_station_name        1104579               18.85\n",
      "  start_station_id        1073884               18.32\n",
      "start_station_name        1073884               18.32\n",
      "           end_lat           7213                0.12\n",
      "           end_lng           7213                0.12\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create a list to store missing value information\n",
    "missing_info = []\n",
    "\n",
    "# Loop through each column\n",
    "for column_name in df.columns:\n",
    "    # Count missing values\n",
    "    missing_count = df[column_name].isnull().sum()\n",
    "    \n",
    "    # Calculate percentage\n",
    "    total_rows = len(df)\n",
    "    missing_percentage = (missing_count / total_rows) * 100\n",
    "    missing_percentage = round(missing_percentage, 2)\n",
    "    \n",
    "    # Store the information\n",
    "    missing_info.append({\n",
    "        'Column': column_name,\n",
    "        'Missing_Count': missing_count,\n",
    "        'Missing_Percentage': missing_percentage\n",
    "    })\n",
    "\n",
    "# Convert to dataframe\n",
    "missing_data = pd.DataFrame(missing_info)\n",
    "\n",
    "# Show only columns with missing values\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0]\n",
    "\n",
    "# Sort by missing count\n",
    "missing_data = missing_data.sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(missing_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a9477",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 6: Handle Missing Values\n",
    "\n",
    "Based on my analysis, I'll keep rows with missing station names because they still have GPS coordinates. However, I'll remove rows missing critical location data (latitude and longitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb6f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing missing coordinates:\n",
      "------------------------------------------------------------\n",
      "Total rows: 5,860,357\n",
      "\n",
      "Removing rows with missing end coordinates...\n",
      "------------------------------------------------------------\n",
      "After removing missing coordinates:\n",
      "Total rows: 5,853,144\n",
      "Rows removed: 7,213\n"
     ]
    }
   ],
   "source": [
    "# Check current size\n",
    "print(\"Before removing missing coordinates:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "\n",
    "# Store original count\n",
    "original_count = len(df)\n",
    "\n",
    "# Remove rows where end_lat or end_lng is missing\n",
    "print(\"\\nRemoving rows with missing end coordinates...\")\n",
    "print(\"-\" * 60)\n",
    "df = df.dropna(subset=['end_lat', 'end_lng'])\n",
    "\n",
    "# Show results\n",
    "print(\"After removing missing coordinates:\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Rows removed: {original_count - len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e08c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 7: Analyze Ride Length Distribution\n",
    "\n",
    "Before filtering, I want to understand the distribution of ride lengths to identify any anomalies or outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd6dcb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ride Length Statistics:\n",
      "------------------------------------------------------------\n",
      "count    5.853144e+06\n",
      "mean     1.446152e+01\n",
      "std      1.317905e+03\n",
      "min     -6.864000e+02\n",
      "25%      5.016667e+00\n",
      "50%      8.316667e+00\n",
      "75%      1.408333e+01\n",
      "max      1.054819e+06\n",
      "Name: ride_length, dtype: float64\n",
      "\n",
      "Mean: 14.46 minutes\n",
      "Median: 8.32 minutes\n",
      "Min: -686.40 minutes\n",
      "Max: 1054819.33 minutes\n",
      "\n",
      "Negative or zero ride lengths: 105\n",
      "Rides over 24 hours (1440 min): 8,096\n"
     ]
    }
   ],
   "source": [
    "# Display statistics for ride_length\n",
    "print(\"Ride Length Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "print(df['ride_length'].describe())\n",
    "\n",
    "# Display key metrics\n",
    "mean_minutes = df['ride_length'].mean()\n",
    "median_minutes = df['ride_length'].median()\n",
    "min_minutes = df['ride_length'].min()\n",
    "max_minutes = df['ride_length'].max()\n",
    "\n",
    "print(f\"\\nMean: {mean_minutes:.2f} minutes\")\n",
    "print(f\"Median: {median_minutes:.2f} minutes\")\n",
    "print(f\"Min: {min_minutes:.2f} minutes\")\n",
    "print(f\"Max: {max_minutes:.2f} minutes\")\n",
    "\n",
    "# Check for problematic values\n",
    "negative_or_zero = (df['ride_length'] <= 0).sum()\n",
    "over_24_hours = (df['ride_length'] > 1440).sum()  # 1440 minutes = 24 hours\n",
    "\n",
    "print(f\"\\nNegative or zero ride lengths: {negative_or_zero:,}\")\n",
    "print(f\"Rides over 24 hours (1440 min): {over_24_hours:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c36ab28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 8: Remove Invalid Ride Records\n",
    "\n",
    "I need to filter out rides that don't make sense:\n",
    "- Rides with negative or zero duration\n",
    "- Rides shorter than 1 minute (likely false starts)\n",
    "- Rides longer than 24 hours (likely unreturned bikes or data errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1d3f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering invalid rides...\n",
      "------------------------------------------------------------\n",
      "Before filtering: 5,853,144 rides\n",
      "\n",
      "Applying filters:\n",
      "- Removing rides < 1 minute\n",
      "- Removing rides > 1440 minutes (24 hours)\n",
      "\n",
      "After filtering: 4,859,019 rides\n",
      "Rides removed: 994,125\n",
      "Percentage retained: 83.02%\n"
     ]
    }
   ],
   "source": [
    "# Filter out invalid rides\n",
    "print(\"Filtering invalid rides...\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Before filtering: {len(df):,} rides\")\n",
    "\n",
    "# Store original count\n",
    "before_filter = len(df)\n",
    "\n",
    "# Apply filters\n",
    "print(\"\\nApplying filters:\")\n",
    "print(\"- Removing rides < 1 minute\")\n",
    "print(\"- Removing rides > 1440 minutes (24 hours)\")\n",
    "\n",
    "# Keep only rides between 1 minute and 1440 minutes (24 hours)\n",
    "df = df[(df['ride_length'] >= 1) & (df['ride_length'] <= 1440)]\n",
    "\n",
    "# Show results\n",
    "after_filter = len(df)\n",
    "rides_removed = before_filter - after_filter\n",
    "percentage_retained = (after_filter / before_filter) * 100\n",
    "\n",
    "print(f\"\\nAfter filtering: {after_filter:,} rides\")\n",
    "print(f\"Rides removed: {rides_removed:,}\")\n",
    "print(f\"Percentage retained: {percentage_retained:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff9dcb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 9: Verify Cleaned Data Quality\n",
    "\n",
    "Now I'll verify that the filtering worked correctly and examine the final statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87d43b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Ride Length Statistics:\n",
      "------------------------------------------------------------\n",
      "count    4.859019e+06\n",
      "mean     9.680817e+00\n",
      "std      5.524413e+00\n",
      "min      1.000000e+00\n",
      "25%      5.250000e+00\n",
      "50%      8.516617e+00\n",
      "75%      1.324084e+01\n",
      "max      2.400000e+01\n",
      "Name: ride_length, dtype: float64\n",
      "\n",
      "Mean: 9.68 minutes\n",
      "Median: 8.52 minutes\n",
      "Min: 1.00 minutes\n",
      "Max: 24.00 minutes\n"
     ]
    }
   ],
   "source": [
    "# Check cleaned ride length statistics\n",
    "print(\"Cleaned Ride Length Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "print(df['ride_length'].describe())\n",
    "\n",
    "# Display key metrics\n",
    "mean_value = df['ride_length'].mean()\n",
    "median_value = df['ride_length'].median()\n",
    "min_value = df['ride_length'].min()\n",
    "max_value = df['ride_length'].max()\n",
    "\n",
    "print(f\"\\nMean: {mean_value:.2f} minutes\")\n",
    "print(f\"Median: {median_value:.2f} minutes\")\n",
    "print(f\"Min: {min_value:.2f} minutes\")\n",
    "print(f\"Max: {max_value:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34e55c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentiles:\n",
      "------------------------------------------------------------\n",
      "25th percentile: 5.25 minutes\n",
      "50th percentile: 8.52 minutes\n",
      "75th percentile: 13.24 minutes\n",
      "90th percentile: 18.19 minutes\n",
      "95th percentile: 20.68 minutes\n",
      "99th percentile: 23.25 minutes\n"
     ]
    }
   ],
   "source": [
    "# Show percentiles for better understanding\n",
    "print(\"\\nPercentiles:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "percentiles_list = [25, 50, 75, 90, 95, 99]\n",
    "\n",
    "for p in percentiles_list:\n",
    "    # Calculate the percentile value\n",
    "    percentile_value = p / 100\n",
    "    value = df['ride_length'].quantile(percentile_value)\n",
    "    print(f\"{p}th percentile: {value:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c3f5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ride Length Distribution:\n",
      "------------------------------------------------------------\n",
      "                  Count  Percentage\n",
      "duration_range                     \n",
      "1-5 min         1113574       22.92\n",
      "5-10 min        1767371       36.37\n",
      "10-15 min       1070173       22.02\n",
      "15-30 min        907901       18.68\n",
      "30-60 min             0        0.00\n",
      "1-2 hrs               0        0.00\n",
      "2-4 hrs               0        0.00\n",
      "4-8 hrs               0        0.00\n",
      "8-24 hrs              0        0.00\n"
     ]
    }
   ],
   "source": [
    "# Analyze distribution by duration range\n",
    "print(\"\\nRide Length Distribution:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Define bins and labels for grouping\n",
    "bins = [1, 5, 10, 15, 30, 60, 120, 240, 480, 1440]\n",
    "labels = ['1-5 min', '5-10 min', '10-15 min', '15-30 min', '30-60 min', \n",
    "          '1-2 hrs', '2-4 hrs', '4-8 hrs', '8-24 hrs']\n",
    "\n",
    "# Create duration range categories\n",
    "df['duration_range'] = pd.cut(df['ride_length'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Count rides in each category\n",
    "distribution = df['duration_range'].value_counts().sort_index()\n",
    "\n",
    "# Calculate percentages\n",
    "total_rides = len(df)\n",
    "distribution_pct = (distribution / total_rides) * 100\n",
    "distribution_pct = distribution_pct.round(2)\n",
    "\n",
    "# Combine into one table\n",
    "result = pd.DataFrame({\n",
    "    'Count': distribution,\n",
    "    'Percentage': distribution_pct\n",
    "})\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Remove the temporary duration_range column\n",
    "df = df.drop('duration_range', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b2c4a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 10: Save Cleaned Dataset\n",
    "\n",
    "Now that the data is clean and ready for analysis, I'll save it to a new file in the processed data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97b95f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved successfully\n",
      "------------------------------------------------------------\n",
      "File path: ../data/processed/cleaned_2024_data.csv\n",
      "Records: 4,859,019\n",
      "Date range: 2024-01-01 00:00:39 to 2024-12-31 23:54:37.045000\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned data\n",
    "output_path = '../data/processed/cleaned_2024_data.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Confirm save was successful\n",
    "print(\"Cleaned dataset saved successfully\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"File path: {output_path}\")\n",
    "print(f\"Records: {len(df):,}\")\n",
    "\n",
    "# Show date range of the cleaned data\n",
    "min_date = df['started_at'].min()\n",
    "max_date = df['started_at'].max()\n",
    "print(f\"Date range: {min_date} to {max_date}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
